{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this nootebook can be found [here](https://inclass.kaggle.com/c/si650winter11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_data = pd.read_csv('kaggle_sentiment.txt', sep='\\t')\n",
    "sentiment_data.columns =['Class', 'Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unlabeld_data = pd.read_csv('unlabeld_data.txt', sep='\\t')\n",
    "unlabeld_data.columns = ['Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>that's not even an exaggeration ) and at midni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                               Data\n",
       "0      1  this was the first clive cussler i've ever rea...\n",
       "1      1                   i liked the Da Vinci Code a lot.\n",
       "2      1                   i liked the Da Vinci Code a lot.\n",
       "3      1  I liked the Da Vinci Code but it ultimatly did...\n",
       "4      1  that's not even an exaggeration ) and at midni..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harvard is dumb, i mean they really have to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm loving Shanghai &gt; &gt; &gt; ^ _ ^.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>harvard is for dumb people.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As i stepped out of my beautiful Toyota, i hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bodies being dismembered, blown apart, and mut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Data\n",
       "0  harvard is dumb, i mean they really have to be...\n",
       "1                   I'm loving Shanghai > > > ^ _ ^.\n",
       "2                        harvard is for dumb people.\n",
       "3  As i stepped out of my beautiful Toyota, i hea...\n",
       "4  Bodies being dismembered, blown apart, and mut..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeld_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1 Shuffle dataframe\n",
    "\n",
    "The dataset is well sorted. First, we have half of data samples that are positive and then half of them negative. If we separate the dataset to training and testing parts like this, we will have most of the data (if not all) from one class. To prevent that from happening, we will shuffle the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "sentiment_data = shuffle(sentiment_data)\n",
    "unlabeld_data = shuffle(unlabeld_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>1</td>\n",
       "      <td>the people who are worth it know how much i lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>1</td>\n",
       "      <td>the people who are worth it know how much i lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>1</td>\n",
       "      <td>i love kirsten / leah / kate escapades and mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3907</th>\n",
       "      <td>1</td>\n",
       "      <td>man i loved brokeback mountain!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>1</td>\n",
       "      <td>I like Mission Impossible movies because you n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class                                               Data\n",
       "749       1  the people who are worth it know how much i lo...\n",
       "339       1  the people who are worth it know how much i lo...\n",
       "1745      1  i love kirsten / leah / kate escapades and mis...\n",
       "3907      1                    man i loved brokeback mountain!\n",
       "1560      1  I like Mission Impossible movies because you n..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2 Split to labels and reviews\n",
    "\n",
    "In this step we need to create separated variables that will hold labels (positive or negative) and reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = sentiment_data.iloc[:, 0].values\n",
    "reviews = sentiment_data.iloc[:, 1].values\n",
    "unlabeled_reviews = unlabeld_data.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.3 Clean data from punctuation\n",
    "\n",
    "The punctuation won't effect our prediction so we will delete all punctuation from reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_processed = []\n",
    "unlabeled_processed = [] \n",
    "for review in reviews:\n",
    "    review_cool_one = ''.join([char for char in review if char not in punctuation])\n",
    "    reviews_processed.append(review_cool_one)\n",
    "    \n",
    "for review in unlabeled_reviews:\n",
    "    review_cool_one = ''.join([char for char in review if char not in punctuation])\n",
    "    unlabeled_processed.append(review_cool_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.4 Creating vocabulary, coverting all characters to lower case and spliting each review into words\n",
    "\n",
    "In this step we are creating vocabulary which will be created by using function Counter. Also in this step we will lower all characters in the dataset, we can do this as well because lower/upper case character won't affect prediction results. Lastly, we will split each review to separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_reviews = []\n",
    "word_unlabeled = []\n",
    "all_words = []\n",
    "for review in reviews_processed:\n",
    "    word_reviews.append(review.lower().split())\n",
    "    for word in review.split():\n",
    "        all_words.append(word.lower())\n",
    "\n",
    "for review in unlabeled_processed:\n",
    "    word_unlabeled.append(review.lower().split())\n",
    "    for word in review.split():\n",
    "        all_words.append(word.lower())\n",
    "    \n",
    "counter = Counter(all_words)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.5 Creating vocab_to_int dictionary which will map word with a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_to_int = {word: i for i, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.6 Using vocab_to_int to transform each review to vector of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews_to_ints = []\n",
    "for review in word_reviews:\n",
    "    reviews_to_ints.append([vocab_to_int[word] for word in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unlabeled_to_ints = []\n",
    "\n",
    "for review in word_unlabeled:\n",
    "    unlabeled_to_ints.append([vocab_to_int[word] for word in review])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.7 Check if we have some 0 length reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length 0\n",
      "Max review length 931\n"
     ]
    }
   ],
   "source": [
    "reviews_lens = Counter([len(x) for x in reviews_to_ints])\n",
    "print('Zero-length {}'.format(reviews_lens[0]))\n",
    "print(\"Max review length {}\".format(max(reviews_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.8 Creating word vectors\n",
    "\n",
    "This step can be done in this way: \n",
    "    1. Define sequence length. (250 in this case)\n",
    "    2. Each review shorted then this sequence will be padded (at the beginning) with zeros\n",
    "    3. Each review longer than the sequence length will be shortened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 250\n",
    "\n",
    "features = np.zeros((len(reviews_to_ints), seq_len), dtype=int)\n",
    "for i, review in enumerate(reviews_to_ints):\n",
    "    features[i, -len(review):] = np.array(review)[:seq_len]\n",
    "    \n",
    "features_test = np.zeros((len(unlabeled_to_ints), seq_len), dtype=int)\n",
    "for i, review in enumerate(unlabeled_to_ints):\n",
    "    features_test[i, -len(review):] = np.array(review)[:seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.9 Split into training and testing parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (6400, 250)\n",
      "X_unlabeled shape (28936, 250)\n"
     ]
    }
   ],
   "source": [
    "X_train = features[:6400]\n",
    "y_train = labels[:6400]\n",
    "\n",
    "X_test = features[6400:]\n",
    "y_test = labels[6400:]\n",
    "\n",
    "X_unlabeled = features_test\n",
    "\n",
    "print('X_trian shape {}'.format(X_train.shape))\n",
    "print('X_unlabeled shape {}'.format(X_unlabeled.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done with preprocessing pipeline\n",
    "\n",
    "## Step 2. Defining RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 512 # how many nodes LSTM cells will have\n",
    "number_of_layers = 1 # how many RNN layers the network will use\n",
    "batch_size = 100 # how many reviews we feed at onces\n",
    "learning_rate = 0.001 # learning rate\n",
    "number_of_words = len(vocab_to_int) + 1 #how many unique words do we have in vocab (+1  is used for 0 - padding)\n",
    "dropout_rate = 0.8 \n",
    "embed_size = 300 #how long our word embedings will be\n",
    "epochs = 6 # how many epochs do we use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #Clean the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1 Define placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "targets = tf.placeholder(tf.int32, [None, None], name='targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2 Define embeding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedings = tf.Variable(tf.random_uniform((number_of_words, embed_size), -1, 1))\n",
    "embed = tf.nn.embedding_lookup(word_embedings, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3 Define hidden layer and Dynamic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size)\n",
    "hidden_layer = tf.contrib.rnn.DropoutWrapper(hidden_layer, dropout_rate)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([hidden_layer]*number_of_layers)\n",
    "init_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs, states = tf.nn.dynamic_rnn(cell, embed, initial_state=init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.4 Get the prediction for each review \n",
    "\n",
    "From the last step of our network we get output and use it as a prediction. Than we use that result and compare it with real sentiment for that review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = tf.layers.dense(outputs[:, -1], 1, activation=tf.sigmoid)\n",
    "cost = tf.losses.mean_squared_error(targets, prediction)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.5 Define accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currect_pred = tf.equal(tf.cast(tf.round(prediction), tf.int32), targets)\n",
    "accuracy = tf.reduce_mean(tf.cast(currect_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/6  | Current loss: 0.05040295422077179  | Training accuracy: 93.2500\n",
      "Epoch: 1/6  | Current loss: 0.008079711347818375  | Training accuracy: 98.9844\n",
      "Epoch: 2/6  | Current loss: 0.0035864049568772316  | Training accuracy: 99.6406\n",
      "Epoch: 3/6  | Current loss: 0.0047574774362146854  | Training accuracy: 99.4844\n",
      "Epoch: 4/6  | Current loss: 0.002684194827452302  | Training accuracy: 99.7031\n",
      "Epoch: 5/6  | Current loss: 0.0015217065811157227  | Training accuracy: 99.7812\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    training_accurcy = []\n",
    "    ii = 0\n",
    "    epoch_loss = []\n",
    "    while ii + batch_size <= len(X_train):\n",
    "        X_batch = X_train[ii:ii+batch_size]\n",
    "        y_batch = y_train[ii:ii+batch_size].reshape(-1, 1)\n",
    "        \n",
    "        a, o, _ = session.run([accuracy, cost, optimizer], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "\n",
    "        training_accurcy.append(a)\n",
    "        epoch_loss.append(o)\n",
    "        ii += batch_size\n",
    "    print('Epoch: {}/{}'.format(i, epochs), ' | Current loss: {}'.format(np.mean(epoch_loss)),\n",
    "          ' | Training accuracy: {:.4f}'.format(np.mean(training_accurcy)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "\n",
    "ii = 0\n",
    "while ii + batch_size <= len(X_test):\n",
    "    X_batch = X_test[ii:ii+batch_size]\n",
    "    y_batch = y_test[ii:ii+batch_size].reshape(-1, 1)\n",
    "\n",
    "    a = session.run([accuracy], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "    \n",
    "    test_accuracy.append(a)\n",
    "    ii += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 98.8000%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy is {:.4f}%\".format(np.mean(test_accuracy)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Testing on the unlabeld data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_unlabeled = []\n",
    "ii = 0\n",
    "while ii + batch_size <= len(X_unlabeled):\n",
    "    if ii + batch_size > len(X_unlabeled):\n",
    "        batch_size = len(X_unlabeled) - ii\n",
    "    X_batch = X_unlabeled[ii:ii+batch_size]\n",
    "    y_batch = X_unlabeled[ii:ii+batch_size].reshape(-1, 1)\n",
    "\n",
    "    pred = session.run([prediction], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "    \n",
    "    predictions_unlabeled.append(pred)\n",
    "    ii += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_real = []\n",
    "for i in range(len(predictions_unlabeled)):\n",
    "    for ii in range(len(predictions_unlabeled[i][0])):\n",
    "        if predictions_unlabeled[i][0][ii][0] >= 0.5:\n",
    "            pred_real.append(1)\n",
    "        else:\n",
    "            pred_real.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('predictions.txt', pred_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dataframe = unlabeld_data[:len(pred_real)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luka\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "new_dataframe['Classes'] = pred_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6244</th>\n",
       "      <td>london sucks....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22537</th>\n",
       "      <td>I love the Toyota Prius.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28827</th>\n",
       "      <td>Great job ata soccer..........</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23001</th>\n",
       "      <td>AAA's \" Q \" is catchy and an ear worm, like ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6567</th>\n",
       "      <td>i love shanghai ~ ~ ~ ~ 外滩好像有一家专卖上海纪念品的小店 ~ ~ ~.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17371</th>\n",
       "      <td>+ + + Bruce Willis hat den PrÃ ¤ sident von Ko...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9954</th>\n",
       "      <td>Since then, 25 automakers including Toyota Mot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18476</th>\n",
       "      <td>And as stupid as San Francisco's road system i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8848</th>\n",
       "      <td>Today, when Monkee was backing out of the Milp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6277</th>\n",
       "      <td>Then we had stupid trivia about San Francisco ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27060</th>\n",
       "      <td>I love Tom Cruise!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27630</th>\n",
       "      <td>I loved the Lakers episode where he told that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25743</th>\n",
       "      <td>i love seattle so much.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26800</th>\n",
       "      <td>Since then I've found that Thinkpad has a horr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21726</th>\n",
       "      <td>My MacBook sucks and I am returning it..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28614</th>\n",
       "      <td>AH SHIITE I LOST FIFTY CENTS TO THE SUPERBOWL ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>But Purdue was inept inside the Wisconsin 20 a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9365</th>\n",
       "      <td>I'm a big fan of Lakers, so I kind of have all...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17069</th>\n",
       "      <td>God bless American Airlines..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20818</th>\n",
       "      <td>I love the lakers even tho Trav makes fun of me.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28617</th>\n",
       "      <td>Hanging out w / Jacob and Adam was also fun an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10483</th>\n",
       "      <td>honda day is freakin awesome...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4203</th>\n",
       "      <td>Now I'm walking around looking like the uglies...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22505</th>\n",
       "      <td>I'm a big fan of Lakers, so I kind of have all...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27825</th>\n",
       "      <td>i love my macbook...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19955</th>\n",
       "      <td>Boston SUCKS.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18770</th>\n",
       "      <td>stupid boston...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>Angelina Jolie looks terrible as a blonde...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22695</th>\n",
       "      <td>AAA rocks.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8724</th>\n",
       "      <td>Honda is excellent,'94 and up.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25963</th>\n",
       "      <td>I miss crossing London's bridges ( and am suff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21783</th>\n",
       "      <td>I wanted to stick with the handy dandy Thinkpa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26895</th>\n",
       "      <td>but the fact that it's in Boston kind of kills...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18059</th>\n",
       "      <td>not only do i hate paris hilton for....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17058</th>\n",
       "      <td>There is a Taste of China right by GEICO that ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13857</th>\n",
       "      <td>I really, really hate TOM CRUISE.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14181</th>\n",
       "      <td>i hate the lakers..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5513</th>\n",
       "      <td>Stupid Allstate.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4570</th>\n",
       "      <td>I love the Toyota Prius.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13250</th>\n",
       "      <td>UCLA sucks anyway.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28783</th>\n",
       "      <td>Yep, I'm still in London, which is pretty awes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10092</th>\n",
       "      <td>I love Paris Hilton..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>I absolutely love my MacBook Pro.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5352</th>\n",
       "      <td>we have a boring as shit blue 2005 toyota caro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22771</th>\n",
       "      <td>Lakers suck balls!!!!).</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14881</th>\n",
       "      <td>I need some of that geico balboa stuff..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>I'd like to talk today about how much I hate P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20982</th>\n",
       "      <td>I'm biased, I love Angelina Jolie..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21525</th>\n",
       "      <td>the fact that i love paris hilton or like the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6977</th>\n",
       "      <td>i do i love angelina jolie!..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19491</th>\n",
       "      <td>I'm loving Shanghai &gt; &gt; &gt; ^ _ ^.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22636</th>\n",
       "      <td>That's why I most love the Harvard story:</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14961</th>\n",
       "      <td>I still like Tom Cruise.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23900</th>\n",
       "      <td>but the macbook looks so awesome.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14313</th>\n",
       "      <td>You are a fucking bitch and I think I may hate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18103</th>\n",
       "      <td>Love Story At Harvard [ awesome drama!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8009</th>\n",
       "      <td>Personally, I blame Angelina Jolie, a psycholo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9882</th>\n",
       "      <td>the stupid honda lol or a BUG!..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9718</th>\n",
       "      <td>I so want a MacBook.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21505</th>\n",
       "      <td>Everyone knows that i simply adore Paris Hilto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Data  Classes\n",
       "6244                                    london sucks....        0\n",
       "22537                           I love the Toyota Prius.        1\n",
       "28827                     Great job ata soccer..........        1\n",
       "23001  AAA's \" Q \" is catchy and an ear worm, like ma...        1\n",
       "6567    i love shanghai ~ ~ ~ ~ 外滩好像有一家专卖上海纪念品的小店 ~ ~ ~.        1\n",
       "17371  + + + Bruce Willis hat den PrÃ ¤ sident von Ko...        0\n",
       "9954   Since then, 25 automakers including Toyota Mot...        0\n",
       "18476  And as stupid as San Francisco's road system i...        1\n",
       "8848   Today, when Monkee was backing out of the Milp...        0\n",
       "6277   Then we had stupid trivia about San Francisco ...        0\n",
       "27060                                 I love Tom Cruise!        1\n",
       "27630  I loved the Lakers episode where he told that ...        1\n",
       "25743                            i love seattle so much.        1\n",
       "26800  Since then I've found that Thinkpad has a horr...        0\n",
       "21726           My MacBook sucks and I am returning it..        0\n",
       "28614  AH SHIITE I LOST FIFTY CENTS TO THE SUPERBOWL ...        0\n",
       "2467   But Purdue was inept inside the Wisconsin 20 a...        1\n",
       "9365   I'm a big fan of Lakers, so I kind of have all...        1\n",
       "17069                      God bless American Airlines..        1\n",
       "20818   I love the lakers even tho Trav makes fun of me.        1\n",
       "28617  Hanging out w / Jacob and Adam was also fun an...        1\n",
       "10483                    honda day is freakin awesome...        1\n",
       "4203   Now I'm walking around looking like the uglies...        1\n",
       "22505  I'm a big fan of Lakers, so I kind of have all...        1\n",
       "27825                               i love my macbook...        1\n",
       "19955                                      Boston SUCKS.        0\n",
       "18770                                   stupid boston...        0\n",
       "2426        Angelina Jolie looks terrible as a blonde...        0\n",
       "22695                                         AAA rocks.        1\n",
       "8724                      Honda is excellent,'94 and up.        1\n",
       "...                                                  ...      ...\n",
       "25963  I miss crossing London's bridges ( and am suff...        1\n",
       "21783  I wanted to stick with the handy dandy Thinkpa...        0\n",
       "26895  but the fact that it's in Boston kind of kills...        0\n",
       "18059            not only do i hate paris hilton for....        0\n",
       "17058  There is a Taste of China right by GEICO that ...        0\n",
       "13857                  I really, really hate TOM CRUISE.        0\n",
       "14181                                i hate the lakers..        0\n",
       "5513                                    Stupid Allstate.        0\n",
       "4570                            I love the Toyota Prius.        1\n",
       "13250                                 UCLA sucks anyway.        0\n",
       "28783  Yep, I'm still in London, which is pretty awes...        1\n",
       "10092                              I love Paris Hilton..        1\n",
       "3221                   I absolutely love my MacBook Pro.        1\n",
       "5352   we have a boring as shit blue 2005 toyota caro...        1\n",
       "22771                            Lakers suck balls!!!!).        0\n",
       "14881           I need some of that geico balboa stuff..        1\n",
       "356    I'd like to talk today about how much I hate P...        0\n",
       "20982                I'm biased, I love Angelina Jolie..        1\n",
       "21525  the fact that i love paris hilton or like the ...        1\n",
       "6977                       i do i love angelina jolie!..        1\n",
       "19491                   I'm loving Shanghai > > > ^ _ ^.        1\n",
       "22636          That's why I most love the Harvard story:        1\n",
       "14961                           I still like Tom Cruise.        1\n",
       "23900                  but the macbook looks so awesome.        1\n",
       "14313  You are a fucking bitch and I think I may hate...        0\n",
       "18103             Love Story At Harvard [ awesome drama!        1\n",
       "8009   Personally, I blame Angelina Jolie, a psycholo...        0\n",
       "9882                    the stupid honda lol or a BUG!..        0\n",
       "9718                                I so want a MacBook.        0\n",
       "21505  Everyone knows that i simply adore Paris Hilto...        0\n",
       "\n",
       "[28900 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
